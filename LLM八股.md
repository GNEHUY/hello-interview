# LLM 八股
1.  请详细解释一下 Transformer 模型中的自注意力机制是如何工作的？它为什么比 RNN 更适合处理长序列？
2.  什么是位置编码？在 Transformer 中，为什么它是必需的？请列举至少两种实现方式。
3.  请你详细介绍ROPE，对比绝对位置编码它的优劣势分别是什么？
4.  你知道MHA，MQA，GQA的区别吗？详细解释一下。
5.  请比较一下几种常见的 LLM 架构，例如 Encoder-Only, Decoder-Only, 和 Encoder-Decoder，并说明它们各自最擅长的任务类型。
6.  什么是Scaling Laws？它揭示了模型性能、计算量和数据量之间的什么关系？这对LLM的研发有什么指导意义？
7.  在LLM的推理阶段，有哪些常见的解码策略？请解释 Greedy Search, Beam Search, Top-K Sampling 和 Nucleus Sampling (Top-P) 的原理和优缺点。
8.  什么是词元化？请比较一下 BPE 和 WordPiece 这两种主流的子词切分算法。
9.  你觉得NLP和LLM最大的区别是什么？两者有何共同和不同之处？
10. L1和L2正则化分别是什么，什么场景适合使用呢？
11. “涌现能力”是大型模型中一个备受关注的现象，请问你如何理解这个概念？它通常在模型规模达到什么程度时出现？
12. 激活函数有了解吗，你知道哪些LLM常用的激活函数？为什么选用它？
13. 混合专家模型（MoE）是如何在不显著增加推理成本的情况下，有效扩大模型参数规模的？请简述其工作原理。
14. 在训练一个百或千亿参数级别的 LLM 时，你会面临哪些主要的工程和算法挑战？（例如：显存、通信、训练不稳定性等）
15. 开源框架了解过哪些？Qwen，Deepseek的论文是否有研读过，说一下其中的创新点主要体现在哪？
16. 最近读过哪些LLM比较前沿的论文，聊一下它的相关方法，针对什么问题，提出了什么方法，对比实验有哪些？
